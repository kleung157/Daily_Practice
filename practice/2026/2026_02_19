Date:
02/19/2026

############################################################################################################

Website:
StrataScratch - ID 9661

Difficulty:
Medium

Question Type:
R

Question:
General Assembly - Find the student with the highest efficiency for mathematics
Find the student with the highest efficiency for mathematics?  
Consider only students with at least 1 hour of studying.
The efficiency is defined as the score divided by hours studied.
Output the highest efficiency along with the other data of that student: 
student id, hours studies and the obtained score for mathematics.

Data Dictionary:
Table name = 'sat_scores'
id: numeric (num)
school: character (str)
teacher: character (str)
student_id: numeric (num)
sat_writing: numeric (num)
sat_verbal: numeric (num)
sat_math: numeric (num)
hrs_studied: numeric (num)
average_sat: numeric (num)
love: POSIXct, POSIXt (dt)

Code:
-------------------------------------------------------------------------------
** Solution #1 ** (original attempt)
## Question:
# Find the student with the highest efficiency for mathematics.
# Consider only students with at least 1 hour of studying.
# The efficiency is defined as the score divided by hours studied.
# Output the highest efficiency along with the other data of tha student:
# student id, hours studied, and the obtain score of mathematics

## Input:
# sat_scores

## Output:
# student_id, hrs_studied, sat_math, efficiency

## Import libraries:
#install.packages("tidyverse")
library(tidyverse)

## Load and preview data:
#sat_scores <- read_csv("sat_scores.csv")
scores_df <- data.frame(sat_scores)
scores_df |> head(5)

## Data quality:
# Dimensions - 135 x 10
# Duplicates - 0
# Nulls - hrs_studied(7), love(135)
# Value Counts - id, school, teacher, student_id, hrs_studied
scores_df |> lapply(class) |> unlist() |> enframe(name="index", value="type")

scores_df |> dim()

scores_df |> duplicated() |> sum()

scores_df |> is.na() |> colSums() |> enframe(name="index", value="na_count")

scores_df |> count(id, sort=TRUE)            # all unique values
scores_df |> count(school, sort=TRUE)        # 3 categories
scores_df |> count(teacher, sort=TRUE)       # 8 categories
scores_df |> count(student_id, sort=TRUE)    # all unique values
scores_df |> count(hrs_studied, sort=TRUE)   # multiple repeated values

## Iteration:
# 1. Filter for students with >= 1 hour of studying (hrs_studied)
# 2. Calculate efficiency for each student,
#    efficiency = math score / hrs studied
# 3. Filter for student with highest efficiency
# 4. Select relevant columns

## Result:
result_df <- scores_df |>
    filter(
        # 1. Filter for students with >= 1 hour of studying (hrs_studied)
        !is.na(hrs_studied) &
        hrs_studied >= 1
    ) |>
    mutate(
        # 2. Calculate efficiency for each student,
        #    efficiency = math score / hrs studied
        efficiency = round(sat_math / hrs_studied, 2)
    ) |>
    slice_max(
        # 3. Filter for student with highest efficiency
        efficiency, with_ties=TRUE
    ) |>
    select(
        # 4. Select relevant columns
        student_id, hrs_studied, sat_math, efficiency
    )

result_df

-------------------------------------------------------------------------------
** Solution #2 ** (revised with suggestions)
result_df <- scores_df |>
    filter(
        # 1. Filter for students with >= 1 hour of studying (hrs_studied)
        !is.na(hrs_studied) &
        hrs_studied >= 1
    ) |>
    mutate(
        # 2. Calculate efficiency for each student,
        #    efficiency = math score / hrs studied
        efficiency = sat_math / hrs_studied
    ) |>
    slice_max(
        # 3. Filter for student with highest efficiency
        efficiency, with_ties=TRUE
    ) |>
    mutate(
        efficiency = round(efficiency, 2)
    ) |>
    select(
        # 4. Select relevant columns
        student_id, hrs_studied, sat_math, efficiency
    )

-------------------------------------------------------------------------------

Notes:
- The data quality check revealed all unique values for the id and student_id columns, multiple repeated
  values for the hrs_studied column, and 3 and 8 categories for the school and teacher columns respectively.
- I began my approach to this problem by filtering for students with at least one hour of studying in the
  hrs_studied column using the filter() function. From there, I calculated the efficiency for each student
  by dividing the sat_math and hrs_studied columns using the mutate() and round() functions. Next, I filtered
  for students with the highest efficiency using the slice_max() function. Lastly, I selected the relevant
  output columns using the select() function.

Suggestions and Final Thoughts:
- It is best to round calculated values after the filtering process rather than before, to ensure that decimal
  places are not lost in the comparison of multiple values and to not create artifical ties. This can be done
  using the mutate() function after the slice_max() function as seen in Solution #2.
  ex.
      mutate(efficiency = round(efficiency, 2)
- When filtering for students with at least one hour of studying in the filter() function, this prevents
  any possible division by zero scenarios when calculating the efficiency in the subsequent step.

Solve Duration:
18 minutes

Notes Duration:
5 minutes

Suggestions and Final Thoughts Duration:
10 minutes

############################################################################################################

Website:
StrataScratch - ID 9746

Difficulty:
Medium

Question Type:
Python

Question:
City of San Francisco - Find top crime categories in 2014 based on the number of occurrences
Find top crime categories in 2014 based on the number of occurrences.
Output the number of crime occurrences alongside the corresponding category name.
Order records based on the number of occurrences in descending order

Data Dictionary:
Table name = 'sf_crime_incidents_2014_01'
incidnt_num: float64 (flt)
category: object (str)
descript: object (str)
day_of_week: object (str)
date: datetime64 (dt)
time: datetime64 (dt)
pd_district: object (str)
resolution: object (str)
address: object (str)
lon: float64 (flt)
lat: float64 (flt)
location: object (str)
id: int64 (int)

Code:
-------------------------------------------------------------------------------
** Solution #1 ** (original attempt)
## Question:
# Find top crime categories in 2014 based on the number of occurences.
# Output the number of crime occurrences alongside the corresponding category name.
# Order records based on the number of occurences in descending order.

## Input:
# sf_crime_incidents_2014_01

## Output:
# category, num_occurrences

## Import libraries:
import pandas as pd

## Load and preview data:
#sf_crime_incidents_2014_01 = pd.read_csv("sf_crime_incidents_2014_01.csv")
incidents_df = sf_crime_incidents_2014_01.copy()
incidents_df.head(5)

## Data quality:
# Dimensions - 100 x 13
# Duplicates - 0
# Nulls - 0
# Value Counts - incidnt_num, category, descript, day_of_week, pd_district, 
#                resolution, address, location, id
#incidents_df.info()

incidents_df.shape

incidents_df.duplicated().sum()

incidents_df.isna().sum().reset_index(name="na_count")

#columns = ["incidnt_num", "category", "descript", "day_of_week", "pd_district",
#           "resolution", "address", "location", "id"]

#for col in columns:
#    print(f"-----{col}-----")
#    with pd.option_context("display.max_rows", None, "display.max_columns", None):
#        print(incidents_df[col].value_counts(dropna=False).reset_index(name="frequency"))
#        print("")
    
# all unique values for incidents_num and id
# multiple repeated values for category, day of week, pd_district, resolution, address, location

## Iteration:
# 1. Filter for records in 2014 (date)
# 2. Calculate the number of crime occurrences for each category
# 3. Sort in descending order by number of occurences

## Result:
target_year = 2014

result_df = (
    incidents_df    
    # 1. Filter for records in 2014 (date)
    .loc[lambda x: x["date"].dt.year == target_year]
    # 2. Calculate the number of crime occurrences for each category
    .groupby("category")["id"]
    .size()
    .reset_index(name="num_occurrences")
    # 3. Sort in descending order by number of occurences
    .sort_values(by="num_occurrences", ascending=False)
)

print("Top crime categories in 2014 based on number of occurrrences: ")
result_df

-------------------------------------------------------------------------------
** Solution #2 ** (revised with suggestions)
target_year = 2014

result_df = (
    incidents_df    
    # 1. Filter for records in 2014 (date)
    .loc[lambda x: x["date"].dt.year == target_year]
    # 2. Calculate the number of crime occurrences for each category
    .groupby("category")
    .size()
    .reset_index(name="num_occurrences")
    # 3. Sort in descending order by number of occurences
    .sort_values(by="num_occurrences", ascending=False)
)

-------------------------------------------------------------------------------

Notes:
- The data quality check revealed all unique values for the incidents_num and id columns, and multiple
  repeated values for the category, day_of_week, pd_district, resolution, address, and location columns.
- I started my approach to this problem by filtering for records in 2014 in the date column using the
  lambda x, loc[], and dt.year accessory functions. Next, I calculated the number of crime occurrences for
  each category using the groupby(), size(), and reset_index() columns. Finally, I sorted the results in
  descending order by number of occurrences using the sort_values() function.

Suggestions and Final Thoughts:
- When using the size() function with the groupby() function for aggregation of rows for each category, the
  column with unique rows does not need to be specified but is still good practice for clarity. I couldn't 
  decide whether to use the incdnt_num or id column since they both contain all unique values. I went with
  the id column because it was an integer datatype while the incdnt_num was a float datatype.
  ex.
      incidents_df.groupby("category").size().reset_index(name="num_occurrences")

Solve Duration:
15 minutes

Notes Duration:
5 minutes

Suggestions and Final Thoughts Duration:
5 minutes

############################################################################################################

Website:
StrataScratch - ID 10553

Difficulty:
Medium

Question Type:
SQL (MS SQL Server)

Question:
Amazon - Finding Purchases
Identify returning active users by finding users who made a repeat purchase within 7 days or less of their previous transaction, excluding same-day purchases. 
Output a list of these user_id.

Data Dictionary:
Table name = 'amazon_transactions'
created_at: date (d)
id: bigint (int)
item: varchar (str)
revenue: bigint (int)
user_id: bigint (int)

Code:
-------------------------------------------------------------------------------
** Solution #1 ** (original attempt)
-- Question:
-- Identify returning active users by finding users who made a repeat purchase within 7 days
-- or less of their previous transaction, excluding same-day purchases.
-- Output a list of these user_ids.

-- Input:
-- amazon_transactions

-- Output:
-- user_id

-- Preview data:
SELECT TOP 5* FROM amazon_transactions;

-- Data quality:
-- Dimensions - 100 x 5
-- Duplicates - 0
-- Nulls - 0
-- Value Counts - id, item, user_id
SELECT -- Dimensions and nulls
    SUM(CASE WHEN created_at IS NULL THEN 1 ELSE 0 END) AS col1,
    SUM(CASE WHEN id IS NULL THEN 1 ELSE 0 END) AS col2,
    SUM(CASE WHEN item IS NULL THEN 1 ELSE 0 END) AS col3,
    SUM(CASE WHEN revenue IS NULL THEN 1 ELSE 0 END) AS col4,
    SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) AS col5,
    COUNT(*) AS row_count
FROM amazon_transactions;

SELECT -- Duplicates
    created_at, id, item, revenue, user_id,
    COUNT(*) AS duplicate_count
FROM amazon_transactions
GROUP BY
    created_at, id, item, revenue, user_id
HAVING COUNT(*) > 1;

SELECT -- Value Counts, all unique values
    id,
    COUNT(*) AS frequency
FROM amazon_transactions
GROUP BY id
ORDER BY frequency DESC;

SELECT -- Value Counts, 4 categories
    item,
    COUNT(*) AS frequency
FROM amazon_transactions
GROUP BY item
ORDER BY frequency DESC;

SELECT -- Value Counts, multiple repeated values
    user_id,
    COUNT(*) AS frequency
FROM amazon_transactions
GROUP BY user_id
ORDER BY frequency DESC;

-- Iteration:
-- 1. Exclude same-day purchases for each user
-- 2. Find previous transaction date for each user
-- 3. Calculate number of days within current and previous purchase for each user
-- 4. Filter for users who made repeat purchases within 7 days or less
-- 5. Include only unique active users

WITH UniqueTransactions AS (
    SELECT DISTINCT -- 1. Exclude same-day purchases for each user
        user_id,
        created_at
    FROM 
        amazon_transactions
),
TransactionSummary AS (
    SELECT
        user_id,
        created_at,
        -- 2. Find previous transaction date for each user
        LAG(created_at) OVER(PARTITION BY user_id ORDER BY created_at) AS previous_date,
        -- 3. Calculate number of days within current and previous purchase for each user
        DATEDIFF(day, LAG(created_at) OVER(PARTITION BY user_id ORDER BY created_at), created_at) AS days
    FROM 
        UniqueTransactions
)
SELECT DISTINCT -- 5. Include only unique active users
    user_id
FROM 
    TransactionSummary
WHERE 
    -- 4. Filter for users who made repeat purchases within 7 days or less
    days <= 7
    AND previous_date IS NOT NULL;

-------------------------------------------------------------------------------
** Solution #2 ** (revised with suggestions)
WITH UniqueTransactions AS (
    SELECT DISTINCT -- Exclude same-day purchases for each user
        user_id,
        created_at
    FROM 
        amazon_transactions
),
TransactionSummary AS (
    SELECT 
        user_id,
        created_at,
        -- Find previous transaction date for each user
        LAG(created_at) OVER (PARTITION BY user_id ORDER BY created_at) AS prev_transaction_date
    FROM
        UniqueTransactions
)
SELECT DISTINCT 
    user_id
FROM 
    TransactionSummary
WHERE 
    prev_transaction_date IS NOT NULL
    -- Must be within 7 days (Difference <= 7)
    AND DATEDIFF(day, prev_transaction_date, created_at) <= 7;
    
-------------------------------------------------------------------------------

Notes:
- The data quality check revealed all unique values for the id column, 4 categories in the item column,
  and multiple repeated values in the user_id column.
- My approach to this problem started with excluding same-day purchases for each user using the DISTINCT
  clause on the user_id and created_at columns within a common table expression (CTE) called 
  UniqueTransactions. Next, I queried this CTE and found the previous transaction date for each user using
  the LAG() function. From there, I calculated the number of days within the current and previous purchase
  for each user. These steps were placed into a second CTE named TransactionSummary. Finally, I queried the
  TransactionSummary CTE to filter for users who made repeat purchases within 7 days or less and included
  only unique active users using the DISTINCT clause on the user_id column.

Suggestions and Final Thoughts:
- Halfway through solving the problem, I thought I misread the requirements and looked too deeply into
  another approach where I included the items column to fulfill "repeated" purchases. I had thought that
  repeated purchases meant the same item. So the final query needed to have the same purchases and still
  fall within 7 days. This wasn't the correct solution so I went back to my original approach of looking
  at only transaction dates and overall purchases rather than specific item column purchases.
- The DATEDIFF() function can be used in the WHERE clause for filtering for repeat purchases within 7
  days or less of their previous transaction. Same-day purchases are excluded through this filter too. For
  edge cases, it's helpful to add an IS NOT NULL filter as well.
  ex.
      WHERE 
          prev_transaction_date IS NOT NULL
          AND DATEDIFF(day, prev_transaction_date, created_at) <= 7;
- While using DATEDIFF() in the WHERE function can be faster for performance, having this function in
  a previous CTE step is better for readability and maintability especially if subsequent calculations
  depend on this calculated column.
- The first CTE step of excluding same-day purchases for each user using DISTINCT is helpful for narrowing
  down transactions to only the unique rows. This means less data to process when performing calculations
  down the query.

Solve Duration:
40 minutes

Notes Duration:
5 minutes

Suggestions and Final Thoughts Duration:
25 minutes

############################################################################################################
